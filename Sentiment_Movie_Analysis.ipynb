{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Movie Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxBRw9T1K5k7"
      },
      "source": [
        "## NLTK’s movie_reviews Dataset is used here.\n",
        "\n",
        "Seperate out the positive reviews data in pos_reviews and negative reviews data in neg_reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc6kVthtKyJy",
        "outputId": "4f2efd21-b61b-4b56-e698-2ac994564509"
      },
      "source": [
        "from nltk.corpus import movie_reviews \n",
        "import nltk\n",
        "\n",
        "nltk.download('movie_reviews') # downloading the data set"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iicDbQLxLyld",
        "outputId": "008d5f6b-694b-47ea-dd83-782b24d28779"
      },
      "source": [
        "pos_reviews = [] \n",
        "for ids in movie_reviews.fileids('pos'):\n",
        "    words = movie_reviews.words(ids)\n",
        "    pos_reviews.append(words)\n",
        " \n",
        "neg_reviews = []\n",
        "for ids in movie_reviews.fileids('neg'):\n",
        "    words = movie_reviews.words(ids)\n",
        "    neg_reviews.append(words)\n",
        " \n",
        "# print first positive review item from the pos_reviews list\n",
        "print (\"pos_reviews[0] :\", pos_reviews[0])\n",
        "print()\n",
        "# print first negative review item from the neg_reviews list\n",
        "print (\"neg_reviews[0] :\", neg_reviews[0])\n",
        "print()\n",
        "# print first 20 items of the first item of positive review\n",
        "print (\"pos_reviews[0][:20] :\", pos_reviews[0][:20])\n",
        "print()\n",
        "# print first 20 items of the first item of negative review\n",
        "print (\"neg_reviews[0][:20] :\", neg_reviews[0][:20])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_reviews[0] : ['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n",
            "\n",
            "neg_reviews[0] : ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
            "\n",
            "pos_reviews[0][:20] : ['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',']\n",
            "\n",
            "neg_reviews[0][:20] : ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV9vg5hKMK1Y"
      },
      "source": [
        "\n",
        "## Now, remove stop words and punctuations.\n",
        " > Because, Stop words are those frequently words which do not carry any significant meaning in text analysis. For example, I, me, my, the, a, and, is, are, he, she, we, and etc. Punctuation marks like comma(,), fullstop(.) inverted comma, etc. occur highly in any text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIARYaPwMERc",
        "outputId": "949ec45e-2f22-43fe-8a75-47e07853c0f9"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "import string \n",
        "nltk.download('stopwords')\n",
        "# stopwords of English language because our model will work on English language only\n",
        "_stopwords = stopwords.words('english') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vGdHbrsNZIp"
      },
      "source": [
        "## Create Feature Set\n",
        "Now, lets write a function that will be used to create feature set. The feature set is used to train the classifier.\n",
        "\n",
        "> Bag-of-words feature is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjTDYuymNZvk",
        "outputId": "b6035111-d4ae-4066-dbff-bee20fb0961b"
      },
      "source": [
        "# feature extractor function\n",
        "def bag_of_words(words):\n",
        "    words_clean = []\n",
        " \n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if (word not in _stopwords) and (word not in string.punctuation): \n",
        "            words_clean.append(word)                                    \n",
        "            \n",
        "    words_dictionary = dict([word, True] for word in words_clean)\n",
        "    \n",
        "    return words_dictionary\n",
        " \n",
        "# using dict will remove duplicate words from the words list\n",
        "# note the output: stopword 'the' is also removed\n",
        "print (\"output:\\nbag_of_words(['the', 'the', 'bad', 'bad', 'the', 'good']) =\", \n",
        "       bag_of_words(['the', 'the', 'bad', 'bad', 'the', 'good']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output:\n",
            "bag_of_words(['the', 'the', 'bad', 'bad', 'the', 'good']) = {'bad': True, 'good': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnWgzzZjQA3A"
      },
      "source": [
        "The ***pos_reviews_set*** and ***neg_reviews_set*** contains the positive and negative reviews with label/sentiment **'pos'** and **'neg'** respectively.\n",
        "\n",
        "\n",
        "Which is use to create train and test set.\n",
        "> **Train Set** : is used to train the classifier.<br>\n",
        "**Test Set** : is used to test the classifier to check how accurately it classifies the given text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ne2jDfGN54g"
      },
      "source": [
        "# positive reviews feature set\n",
        "pos_reviews_set = []\n",
        "for words in pos_reviews:\n",
        "    pos_reviews_set.append((bag_of_words(words), 'pos'))\n",
        " \n",
        "# negative reviews feature set\n",
        "neg_reviews_set = []\n",
        "for words in neg_reviews:\n",
        "    neg_reviews_set.append((bag_of_words(words), 'neg'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhuYMy9sVeFz"
      },
      "source": [
        "## Creating Train and Test Set\n",
        "There are 1000 positive reviews set and 1000 negative reviews set.<br> We take 20% (i.e. 200) of positive reviews and 20% (i.e. 200) of negative reviews as a test set. The remaining negative and positive reviews will be taken as a training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "257DNEVOSwcv",
        "outputId": "1d96fb34-806f-4a0e-9cdb-9c2000279947"
      },
      "source": [
        "len(pos_reviews_set), len(neg_reviews_set)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehENaXjiWZKF"
      },
      "source": [
        "# radomize pos_reviews_set and neg_reviews_set\n",
        "# doing so will output different accuracy result everytime we run the program\n",
        "from random import shuffle \n",
        "shuffle(pos_reviews_set)\n",
        "shuffle(neg_reviews_set)\n",
        " \n",
        "test_set = pos_reviews_set[:200] + neg_reviews_set[:200]\n",
        "train_set = pos_reviews_set[200:] + neg_reviews_set[200:]\n",
        "X_count_vect =  pos_reviews_set + neg_reviews_set"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_5gOfdsWmfO",
        "outputId": "015118b3-886f-4538-ff44-ba14d2fc411e"
      },
      "source": [
        "len(test_set), len(train_set)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 1600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIPeHuYlW8YO"
      },
      "source": [
        "## Training Classifier and Calculating Accuracy\n",
        "We train Naive Bayes Classifier using the training set and calculate the classification accuracy of the trained classifier using the test set.\n",
        "> More on Naive Bayes Classifier:<br>\n",
        "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMz4fV0tWqCI",
        "outputId": "ebb6c01c-6dbf-4188-f225-992ef0c54be0"
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        " \n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        " \n",
        "accuracy = classify.accuracy(classifier, test_set)\n",
        "print(\"accuracy :- \", accuracy)\n",
        " \n",
        "print (classifier.show_most_informative_features(10)) # show most 10 informative features"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy :-  0.6825\n",
            "Most Informative Features\n",
            "                  finest = True              pos : neg    =     14.2 : 1.0\n",
            "             outstanding = True              pos : neg    =     13.9 : 1.0\n",
            "               maintains = True              pos : neg    =     13.7 : 1.0\n",
            "               ludicrous = True              neg : pos    =     13.4 : 1.0\n",
            "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
            "               stupidity = True              neg : pos    =     11.0 : 1.0\n",
            "             fascination = True              pos : neg    =     10.3 : 1.0\n",
            "               atrocious = True              neg : pos    =     10.3 : 1.0\n",
            "                   sucks = True              neg : pos    =      9.8 : 1.0\n",
            "              astounding = True              pos : neg    =      9.7 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJub_lrKZDIG"
      },
      "source": [
        "## Observations:\n",
        "Let’s see the most informative features among the entire features in the feature set.<br>\n",
        "The result shows that the word outstanding is used in positive reviews 13.9 times more often than it is used in negative reviews, the word ludicrous is used in negative reviews 13.4 times more often than it is used in positive reviews. Similarly, for other letters. These ratios are also called likelihood ratios.\n",
        "\n",
        "Therefore, a review has a high chance to be classified as positive if it contains words like outstanding and finest, etc. Similarly, a review has a high chance of being classified as negative if it contains words like ludicrous, stupidity, sucks, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zLyH4fiabV2"
      },
      "source": [
        "## Testing Classifier with Custom Review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0p0x9gYn6n",
        "outputId": "0a86fac7-a222-4015-eda9-642a3ad1df70"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt') \n",
        "\n",
        "custom_review = \"Worst movie i have ever seen. It is worst then the corona virus, disasteres.\"\n",
        "custom_review_tokens = word_tokenize(custom_review)\n",
        "custom_review_set = bag_of_words(custom_review_tokens)\n",
        "print (\"Predicition on,\\n'Worst movie i have ever seen. It is worst then the corona virus, disasteres.' custom reviews : \", classifier.classify(custom_review_set))\n",
        "# Negative review correctly classified as negative\n",
        " \n",
        "# probability result\n",
        "prob_result = classifier.prob_classify(custom_review_set)\n",
        "print (prob_result)\n",
        "print (prob_result.max())\n",
        "print (\"probability to classify as 'neg':\", prob_result.prob(\"neg\"))\n",
        "print (\"probability to classigy as 'pos':\", prob_result.prob(\"pos\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Predicition on,\n",
            "'Worst movie i have ever seen. It is worst then the corona virus, disasteres.' custom reviews :  neg\n",
            "<ProbDist with 2 samples>\n",
            "neg\n",
            "probability to classify as 'neg': 0.6302168701132754\n",
            "probability to classigy as 'pos': 0.36978312988672457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drBolqzNaFDT",
        "outputId": "9376bd37-f1fe-494f-b83c-3f21052630a2"
      },
      "source": [
        "custom_review = \"Better then Corona, its good to watch movie like this, wonderful.\"\n",
        "custom_review_tokens = word_tokenize(custom_review)\n",
        "custom_review_set = bag_of_words(custom_review_tokens)\n",
        " \n",
        "print (\"Predicition on,\\n'Better then Corona, its good to watch movie like this, wonderful.' custom reviews : \", classifier.classify(custom_review_set))\n",
        "# Positive review correctly classified as positive\n",
        " \n",
        "# probability result\n",
        "prob_result = classifier.prob_classify(custom_review_set)\n",
        "print (prob_result)\n",
        "print (prob_result.max())\n",
        "print (\"probability to classify as 'neg':\", prob_result.prob(\"neg\"))\n",
        "print (\"probability to classigy as 'pos':\", prob_result.prob(\"pos\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicition on,\n",
            "'Better then Corona, its good to watch movie like this, wonderful.' custom reviews :  pos\n",
            "<ProbDist with 2 samples>\n",
            "pos\n",
            "probability to classify as 'neg': 0.1406255106492366\n",
            "probability to classigy as 'pos': 0.8593744893507645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0xEcR-VaIVO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}